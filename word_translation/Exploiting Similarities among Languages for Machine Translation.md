## [Exploiting Similarities among Languages for Machine Translation](https://arxiv.org/abs/1309.4168)

<p align="justify">
In this paper, the proposed method uses distributed representation of words and learns a linear mapping between vector spaces of the languages. The method consists of two simple steps. **First**, they build monolingual models of languages using large amounts of text. And **second**, they use a small bilingual dictionary to learn a linear projection between the languages. At the test time, they can translate any word that has been seen in the monolingual corpora by projecting its vector representation from the source language space to the target language space. Once the vector is obtained in the target language space, they output the most similar word vector as the translation.
<p align="justify">


#### Key Points

- The similarity of geometric arrangments in vector spaces is the key reason why the method works well.
- Distributed representations of words capture surprisingly many linguistic regularities.
- The method is useful not only for translation between similar languages, but also for the languages that are substantially different (such as English to Czech or English to Chinese).
 - Skip-gram gives better word representations when the monolingual data is small. CBOW however is faster and more suitable for larger datasets.
 - Two baseline techniques: one based on the edit distance between words, and the other based on similarity of word co-occurrences that uses word counts
 - To obtain dictionaries between languages, they used the most frequent words from the monolingual source datasets, and translated these words using on-line Google Translate (GT)
 - Their approach can also be successfully used for translating infrequent word
 - **They observed that the word vectors trained on the source language should be several times (around 2xâ€“4x) larger than the word vectors trained on the target language.**
 
 
 
 #### Notes
 <p align="justify">
They use the most frequent 5K words from the source language and their translations given GT as the training data for learning the Translation Matrix. The subsequent 1K words in the source language and their translations are used as a test set.
*They report the top 5 accuracy in addition to the top 1 accuracy.* It should be noted that the top 1 accuracy is highly underestimated, as synonym translations are counted as mistakes - they count only exact match as a successful translation.
  
If they apply the Translation Matrix to a word vector in English and obtain a vector in the Spanish word space that is not close to vector of any Spanish word, they can assume that the translation is likely to be inaccurate. *If this value is smaller than a threshold, the translation is skipped.*
By adding the edit distance to scores, further im- prove the accuracy can be done.
  
 
 
